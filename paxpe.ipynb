{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAXPE - Ingestão de dados para banco de dados SQL usando Airflow\n",
    "\n",
    "## Visão geral\n",
    "\n",
    "Este notebook demonstra como agendar um script Python para ingerir dados em um Banco de Dados SQL do Postgres, orquestrado pelo airflow.\n",
    "\n",
    "# Documentação do Processo de Criação de Tabelas com Dados do Yahoo Finance\n",
    "\n",
    "## Objetivo\n",
    "O objetivo deste processo é obter dados financeiros, de mercado, dividendos, valuation e informações gerais de empresas listadas na bolsa, utilizando a API do Yahoo Finance. Os dados são coletados para um ou mais tickers e organizados em DataFrames utilizando PySpark para garantir performance e escalabilidade, especialmente ao lidar com uma grande quantidade de tickers.\n",
    "\n",
    "1. **Coleta de Dados**: \n",
    "   - Para cada ticker fornecido, as informações relevantes foram extraídas da API do Yahoo Finance utilizando a biblioteca `yfinance`. Os dados foram organizados em dicionários para posterior conversão em DataFrames.\n",
    "\n",
    "2. **Criação dos DataFrames**:\n",
    "   - **Tabela Geral** (`df_geral`): Contém informações gerais da empresa, como setor, indústria, número de empregados, localização e resumo das atividades.\n",
    "   - **Tabela Financeira** (`df_financeira`): Contém dados financeiros da empresa, como capitalização de mercado, receita, lucro líquido, EBITDA, dívida total, entre outros.\n",
    "   - **Tabela de Mercado** (`df_mercado`): Inclui dados relacionados ao mercado, como preço atual, preço de abertura, volume de negociação, beta, entre outros.\n",
    "   - **Tabela de Dividendos** (`df_dividendos`): Contém informações sobre dividendos, incluindo taxa de dividendos, data ex-dividendo e índice de distribuição.\n",
    "   - **Tabela de Valuation** (`df_valuation`): Inclui dados de valuation da empresa, como índices P/E (Price to Earnings), P/B (Price to Book) e PEG (Price/Earnings to Growth).\n",
    "   - **Tabela de Retorno Mensal** (`df_retorno_mensal`): Retorno mensal da ação com base em preço da ação, dividendos e percentual\n",
    "\n",
    "## Considerações Finais\n",
    "Este processo permite a coleta eficiente e escalável de dados financeiros de várias empresas, facilitando análises complexas em grandes volumes de dados. O uso do PySpark garante que mesmo listas extensas de tickers possam ser processadas rapidamente, gerando tabelas estruturadas e prontas para análise.\n",
    "\n",
    "\n",
    "# Changelog\n",
    "\n",
    "| Responsável | Data       | Change Log                                                                                      |\n",
    "|-------------|------------|--------------------------------------------------------------------------------------------------|\n",
    "| IGOR MENDES | 10-08-24 | Criação do script em spark                   |\n",
    "| IGOR MENDES | 28-08-24 | Criação da logica de upsert com o postgresSQL                |\n",
    "| IGOR MENDES | 20-10-24 | Sprint 3 - adicionando indices a um dataframe               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yahoofinance in c:\\spark\\python 3.10\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\spark\\python 3.10\\lib\\site-packages (from yahoofinance) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.3 in c:\\spark\\python 3.10\\lib\\site-packages (from yahoofinance) (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20.1 in c:\\spark\\python 3.10\\lib\\site-packages (from yahoofinance) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4>=4.6.3->yahoofinance) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4>=4.6.3->yahoofinance) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=0.23.4->yahoofinance) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=0.23.4->yahoofinance) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=0.23.4->yahoofinance) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=0.23.4->yahoofinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.20.1->yahoofinance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.20.1->yahoofinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.20.1->yahoofinance) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.20.1->yahoofinance) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\spark\\python 3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->yahoofinance) (1.16.0)\n",
      "Requirement already satisfied: yahooquery in c:\\spark\\python 3.10\\lib\\site-packages (2.3.7)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (4.13.3)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (4.9.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.3 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (2.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (2.32.3)\n",
      "Requirement already satisfied: requests-futures<2.0.0,>=1.0.1 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in c:\\spark\\python 3.10\\lib\\site-packages (from yahooquery) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.2->yahooquery) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.2->yahooquery) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas<3.0.0,>=2.0.3->yahooquery) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas<3.0.0,>=2.0.3->yahooquery) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas<3.0.0,>=2.0.3->yahooquery) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas<3.0.0,>=2.0.3->yahooquery) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\spark\\python 3.10\\lib\\site-packages (from requests<3.0.0,>=2.31.0->yahooquery) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\spark\\python 3.10\\lib\\site-packages (from requests<3.0.0,>=2.31.0->yahooquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\spark\\python 3.10\\lib\\site-packages (from requests<3.0.0,>=2.31.0->yahooquery) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\spark\\python 3.10\\lib\\site-packages (from requests<3.0.0,>=2.31.0->yahooquery) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\spark\\python 3.10\\lib\\site-packages (from tqdm<5.0.0,>=4.65.0->yahooquery) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\spark\\python 3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.3->yahooquery) (1.16.0)\n",
      "Requirement already satisfied: yfinance in c:\\spark\\python 3.10\\lib\\site-packages (0.2.55)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (1.24.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (2.4.4)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (3.17.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\spark\\python 3.10\\lib\\site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\spark\\python 3.10\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\spark\\python 3.10\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.31->yfinance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\spark\\python 3.10\\lib\\site-packages (from requests>=2.31->yfinance) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\spark\\python 3.10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\spark\\python 3.10\\lib\\site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "#fontes - yahoo finance api\n",
    "!pip install yahoofinance\n",
    "!pip install yahooquery\n",
    "!pip install --upgrade yfinance\n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()  # Inicializa o Spark\n",
    "findspark.find()  # Verifica se o Spark está corretamente configurado\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"PAXPE\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")  # Define o fuso horário para São Paulo\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Memória do driver\n",
    "    .config(\"spark.executor.memory\", \"12g\")  # Memória para cada executor (ajuste conforme a carga)\n",
    "    .config(\"spark.executor.cores\", \"8\")  # Núcleos por executor\n",
    "    .config(\"spark.cores.max\", \"24\")  # Total de núcleos disponíveis\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"10\")\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"24\")  # Nível de paralelismo\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Memória usada para armazenamento e execução\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")  # Memória usada para armazenamento\n",
    "    .config(\"spark.jars\", \"/opt/airflow/jars/postgresql-42.7.4.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "from yfinance import Ticker\n",
    "#api yahoo\n",
    "from yahooquery import Screener, Ticker\n",
    "\n",
    "\n",
    "#criar timestamps e automatizar a safra de tempo da análise\n",
    "# apagar depois que tiver usando a api do spark sql\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, lag, current_timestamp , date_format, from_utc_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, LongType, DateType, DoubleType,IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_empresas_ativas():\n",
    "    screener = Screener()\n",
    "    dados = screener.get_screeners('most_actives', count=200)\n",
    "    # print(dados)  # Linha de depuração para inspecionar a estrutura dos dados retornados\n",
    "    empresas = dados['most_actives']['quotes']\n",
    "    \n",
    "    # Criar um DataFrame a partir dos dados\n",
    "    df = spark.createDataFrame(empresas)\n",
    "    \n",
    "    # Colunas para corresponder ao site\n",
    "    colunas = [\n",
    "        'symbol', 'shortName', 'displayName', 'regularMarketPrice', 'regularMarketChange', \n",
    "        'regularMarketChangePercent', 'regularMarketVolume', 'marketCap', \n",
    "        'fullExchangeName', 'quoteSourceName'\n",
    "    ]\n",
    "    df = df.select(*colunas)\n",
    "    \n",
    "    # Renomear colunas para português\n",
    "    df = df.withColumnRenamed('symbol', 'ticker') \\\n",
    "           .withColumnRenamed('shortName', 'nome_curto') \\\n",
    "           .withColumnRenamed('displayName', 'nome_exibicao') \\\n",
    "           .withColumnRenamed('regularMarketPrice', 'preco_mercado_regular') \\\n",
    "           .withColumnRenamed('regularMarketChange', 'mudanca_mercado_regular') \\\n",
    "           .withColumnRenamed('regularMarketChangePercent', 'mudanca_percentual_mercado_regular') \\\n",
    "           .withColumnRenamed('regularMarketVolume', 'volume_mercado_regular') \\\n",
    "           .withColumnRenamed('marketCap', 'capitalizacao_mercado') \\\n",
    "           .withColumnRenamed('fullExchangeName', 'nome_exchange_completa') \\\n",
    "           .withColumnRenamed('quoteSourceName', 'nome_fonte_cotacao')\n",
    "    \n",
    "    # Adicionar coluna com data e hora atual\n",
    "    df = df.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df = df.withColumn('dthr_igtao', current_timestamp())\n",
    "    \n",
    "    # Garantir que 'ticker' não tenha valores nulos\n",
    "    df = df.withColumn('ticker', col('ticker').cast('string'))\n",
    "    df = df.dropna(subset=['ticker'])\n",
    "\n",
    "    # Ordenar por capitalizacaoMercado\n",
    "    df = df.orderBy(col('capitalizacao_mercado').desc())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_dados_historicos(symbols, start_date, end_date):\n",
    "    dados = {}\n",
    "    for symbol in symbols:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        historico = ticker.history(start=start_date, end=end_date, interval='1mo')\n",
    "        dados[symbol] = historico\n",
    "    \n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def obter_dados_historicos_ticker(symbol, start_date, end_date):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    historico = ticker.history(start=start_date, end=end_date, interval='1mo')\n",
    "    return (symbol, historico)\n",
    "\n",
    "def obter_dados_historicos(symbols, start_date, end_date):\n",
    "    dados = {}\n",
    "    \n",
    "    # Usar ThreadPoolExecutor para executar as solicitações em paralelo\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit tarefas para o executor\n",
    "        futuros = [executor.submit(obter_dados_historicos_ticker, symbol, start_date, end_date) for symbol in symbols]\n",
    "        \n",
    "        # Coletar os resultados conforme as tarefas são concluídas\n",
    "        for futuro in as_completed(futuros):\n",
    "            symbol, historico = futuro.result()\n",
    "            dados[symbol] = historico\n",
    "    \n",
    "    return dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorno_mensal(dados):\n",
    "    # Inicializar uma lista vazia para armazenar dados estruturados\n",
    "    dados_estruturados = []\n",
    "    \n",
    "    # Iterar sobre os dados históricos de cada símbolo\n",
    "    for symbol, df in dados.items():\n",
    "        # Converter DataFrame do Pandas para PySpark\n",
    "        df['symbol'] = symbol\n",
    "\n",
    "        df_spark = spark.createDataFrame(df.reset_index())\n",
    "        \n",
    "        \n",
    "        # Renomear colunas para português\n",
    "        df_spark = df_spark.withColumnRenamed('symbol', 'ticker') \\\n",
    "                        .withColumnRenamed('Date', 'data') \\\n",
    "                        .withColumnRenamed('Open', 'abertura') \\\n",
    "                        .withColumnRenamed('High', 'alta') \\\n",
    "                        .withColumnRenamed('Low', 'baixa') \\\n",
    "                        .withColumnRenamed('Close', 'fechamento') \\\n",
    "                        .withColumnRenamed('Volume', 'volume') \\\n",
    "                        .withColumnRenamed('Dividends', 'dividendos') \\\n",
    "                        .withColumnRenamed('Stock Splits', 'desdobramentos')\n",
    "\n",
    "        janela = Window.partitionBy('ticker').orderBy('Data')\n",
    "\n",
    "\n",
    "        # Calcular preço de fechamento do mês anterior (deslocar uma linha para cima)\n",
    "        df_spark = df_spark.withColumn('fechamento_mes_anterior', lag('fechamento').over(janela))\n",
    "\n",
    "        # Calcular Retorno em valor (diferença absoluta)\n",
    "        df_spark = df_spark.withColumn('valor_retorno', col('fechamento') - col('fechamento_mes_anterior'))\n",
    "\n",
    "        # Calcular Retorno em porcentagem\n",
    "        df_spark = df_spark.withColumn('porcentagem_retorno', (col('valor_retorno') / col('fechamento_mes_anterior')) * 100)\n",
    "\n",
    "        # Adicionar coluna com data atual no formato desejado\n",
    "        df_spark = df_spark.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "        df_spark = df_spark.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "        # Reordenar colunas\n",
    "        df_spark = df_spark.select(\n",
    "            'ticker',               # 'symbol' traduzido para 'ticker'\n",
    "            'data',                 # 'date' traduzido para 'Data'\n",
    "            'abertura',             # 'open' traduzido para 'Abertura'\n",
    "            'alta',                 # 'high' traduzido para 'Alta'\n",
    "            'baixa',                # 'low' traduzido para 'Baixa'\n",
    "            'fechamento',           # 'close' traduzido para 'Fechamento'\n",
    "            'volume',               # 'volume' mantido como 'Volume'\n",
    "            'dividendos',           # 'dividends' traduzido para 'Dividendos'\n",
    "            'desdobramentos',       # 'splits' traduzido para 'Desdobramentos'\n",
    "            'fechamento_mes_anterior', # 'Close_Last_Month' traduzido para 'Fechamento_Mes_Anterior'\n",
    "            'valor_retorno',        # 'Return_Value' traduzido para 'Valor_Retorno'\n",
    "            'porcentagem_retorno',  # 'Return_Percentage' traduzido para 'Porcentagem_Retorno'\n",
    "            'dt_ptcao',             # 'dt_ptcao' mantido como está\n",
    "            'dthr_igtao'            # 'DTHR_IGTAO' mantido como está\n",
    "        )\n",
    "        \n",
    "        # Adicionar o DataFrame à lista de dados estruturados\n",
    "        dados_estruturados.append(df_spark)\n",
    "\n",
    "    # Unir todos os DataFrames em um único DataFrame\n",
    "    df_final = dados_estruturados[0]\n",
    "    for df in dados_estruturados[1:]:\n",
    "        df_final = df_final.union(df)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabela fato -  maiores empresas segundo a api do yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ativas = obter_empresas_ativas()\n",
    "\n",
    "df_ativas.show()\n",
    "df_ativas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensão - Retornos mensais 10 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 anos passados\n",
    "start_date = datetime.today() - timedelta(days=10*365)\n",
    "\n",
    "# hoje\n",
    "end_date = datetime.today()\n",
    "\n",
    "# 'YYYY-MM-DD'\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"start_date: {start_date_str}\")\n",
    "print(f\"end_date: {end_date_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar a coluna 'symbol' e coletar os valores como uma lista\n",
    "#100 maiores para dimensão das 100 maiores e retornos\n",
    "symbol_list = df_ativas.select('ticker').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Converter a lista para uma tupla\n",
    "df_tickers = tuple(symbol_list)\n",
    "\n",
    "print(df_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = obter_dados_historicos(df_tickers, start_date_str, end_date_str)\n",
    "df_retorno_mensal = retorno_mensal(historical_data)\n",
    "\n",
    "# Mostrar o DataFrame final\n",
    "df_retorno_mensal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimensões gerais \n",
    "\n",
    "2. **Criação dos DataFrames**:\n",
    "   - **Tabela Geral** (`df_financeira`): Contém informações gerais da empresa, como setor, indústria, número de empregados, localização e resumo das atividades.\n",
    "   - **Tabela Financeira** (`df_financeira`): Contém dados financeiros da empresa, como capitalização de mercado, receita, lucro líquido, EBITDA, dívida total, entre outros.\n",
    "   - **Tabela de Mercado** (`df_mercado`): Inclui dados relacionados ao mercado, como preço atual, preço de abertura, volume de negociação, beta, entre outros.\n",
    "   - **Tabela de Dividendos** (`df_dividendos`): Contém informações sobre dividendos, incluindo taxa de dividendos, data ex-dividendo e índice de distribuição.\n",
    "   - **Tabela de Valuation** (`df_valuation`): Inclui dados de valuation da empresa, como índices P/E (Price to Earnings), P/B (Price to Book) e PEG (Price/Earnings to Growth).\n",
    "   - **Tabela de Retorno Mensal** (`df_retorno_mensal`): Retorno mensal da ação com base em preço da ação, dividendos e percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "def criar_tabelas_spark(tickers):\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = (tickers,)\n",
    "    \n",
    "    # Esquema para a tabela geral\n",
    "    schema_geral = StructType([\n",
    "        StructField('ticker', StringType(), False),\n",
    "        StructField('setor', StringType(), True),\n",
    "        StructField('industria', StringType(), True),\n",
    "        StructField('funcionarios', IntegerType(), True),\n",
    "        StructField('cidade', StringType(), True),\n",
    "        StructField('estado', StringType(), True),\n",
    "        StructField('pais', StringType(), True),\n",
    "        StructField('website', StringType(), True),\n",
    "        StructField('resumo_negocios', StringType(), True),\n",
    "        StructField('exchange', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Esquema para a tabela financeira\n",
    "    schema_financeira = StructType([\n",
    "        StructField('ticker', StringType(), False),\n",
    "        StructField('capitalizacao_mercado', LongType(), True),\n",
    "        StructField('valor_empresa', LongType(), True),\n",
    "        StructField('receita', LongType(), True),\n",
    "        StructField('lucros_brutos', LongType(), True),\n",
    "        StructField('lucro_liquido', LongType(), True),\n",
    "        StructField('ebitda', LongType(), True),\n",
    "        StructField('divida_total', LongType(), True),\n",
    "        StructField('caixa_total', LongType(), True),\n",
    "        StructField('dividend_yield', DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Esquema para a tabela de mercado\n",
    "    schema_mercado = StructType([\n",
    "        StructField('ticker', StringType(), False),\n",
    "        StructField('preco_atual', DoubleType(), True),\n",
    "        StructField('fechamento_anterior', DoubleType(), True),\n",
    "        StructField('abertura', DoubleType(), True),\n",
    "        StructField('minimo_dia', DoubleType(), True),\n",
    "        StructField('maximo_dia', DoubleType(), True),\n",
    "        StructField('minimo_52_semanas', DoubleType(), True),\n",
    "        StructField('maximo_52_semanas', DoubleType(), True),\n",
    "        StructField('volume', LongType(), True),\n",
    "        StructField('volume_medio', LongType(), True),\n",
    "        StructField('beta', DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Esquema para a tabela de dividendos\n",
    "    schema_dividendos = StructType([\n",
    "        StructField('ticker', StringType(), False),\n",
    "        StructField('taxa_dividendo', DoubleType(), True),\n",
    "        StructField('data_exdividendo', StringType(), True),  # Temporariamente como StringType\n",
    "        StructField('indice_distribuicao', DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Esquema para a tabela de valuation\n",
    "    schema_valuation = StructType([\n",
    "        StructField('ticker', StringType(), False),\n",
    "        StructField('pl_futuro', DoubleType(), True),\n",
    "        StructField('pl_retroativo', DoubleType(), True),\n",
    "        StructField('preco_booking', DoubleType(), True),\n",
    "        StructField('indice_preco_lucro_cresc', DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Inicializa as listas de dicionários para cada tabela\n",
    "    geral = []\n",
    "    financeira = []\n",
    "    mercado = []\n",
    "    dividendos = []\n",
    "    valuation = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            empresa = yf.Ticker(ticker)\n",
    "            info = empresa.info\n",
    "            \n",
    "            # Filtra e trata valores infinitos\n",
    "            def safe_get(key, default=None):\n",
    "                value = info.get(key)\n",
    "                if isinstance(value, str) and value in ('Infinity', '-Infinity'):\n",
    "                    return default\n",
    "                return value\n",
    "            \n",
    "            # Preencher dados da tabela geral\n",
    "            geral.append({\n",
    "                'ticker': ticker,\n",
    "                'setor': info.get('sector'),\n",
    "                'industria': info.get('industry'),\n",
    "                'funcionarios': info.get('fullTimeEmployees'),\n",
    "                'cidade': info.get('city'),\n",
    "                'estado': info.get('state'),\n",
    "                'pais': info.get('country'),\n",
    "                'website': info.get('website'),\n",
    "                'resumo_negocios': info.get('longBusinessSummary'),\n",
    "                'exchange': info.get('exchange')\n",
    "            })\n",
    "            \n",
    "            # Preencher dados da tabela financeira\n",
    "            financeira.append({\n",
    "                'ticker': ticker,\n",
    "                'capitalizacao_mercado': safe_get('marketCap', 0),\n",
    "                'valor_empresa': safe_get('enterpriseValue', 0),\n",
    "                'receita': safe_get('revenue', 0),\n",
    "                'lucros_brutos': safe_get('grossProfits', 0),\n",
    "                'lucro_liquido': safe_get('netIncome', 0),\n",
    "                'ebitda': safe_get('ebitda', 0),\n",
    "                'divida_total': safe_get('totalDebt', 0),\n",
    "                'caixa_total': safe_get('totalCash', 0),\n",
    "                'dividend_yield': safe_get('dividendYield', 0.0)\n",
    "            })\n",
    "            \n",
    "            # Preencher dados da tabela de mercado\n",
    "            mercado.append({\n",
    "                'ticker': ticker,\n",
    "                'preco_atual': safe_get('currentPrice', 0.0),\n",
    "                'fechamento_anterior': safe_get('previousClose', 0.0),\n",
    "                'abertura': safe_get('open', 0.0),\n",
    "                'minimo_dia': safe_get('dayLow', 0.0),\n",
    "                'maximo_dia': safe_get('dayHigh', 0.0),\n",
    "                'minimo_52_semanas': safe_get('fiftyTwoWeekLow', 0.0),\n",
    "                'maximo_52_semanas': safe_get('fiftyTwoWeekHigh', 0.0),\n",
    "                'volume': safe_get('volume', 0),\n",
    "                'volume_medio': safe_get('averageVolume', 0),\n",
    "                'beta': safe_get('beta', 0.0)\n",
    "            })\n",
    "            \n",
    "            # Preencher dados da tabela de dividendos\n",
    "            dividendos.append({\n",
    "                'ticker': ticker,\n",
    "                'taxa_dividendo': safe_get('dividendRate', 0.0),\n",
    "                'data_exdividendo': safe_get('exDividendDate'),  # Unix timestamp\n",
    "                'indice_distribuicao': safe_get('payoutRatio', 0.0)\n",
    "            })\n",
    "            \n",
    "            # Preencher dados da tabela de valuation\n",
    "            valuation.append({\n",
    "                'ticker': ticker,\n",
    "                'pl_futuro': safe_get('forwardPE', 0.0),\n",
    "                'pl_retroativo': safe_get('trailingPE', 0.0),\n",
    "                'preco_booking': safe_get('priceToBook', 0.0),\n",
    "                'indice_preco_lucro_cresc': safe_get('pegRatio', 0.0)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o ticker {ticker}: {e}\")\n",
    "    \n",
    "    # Criar DataFrames Spark com esquema definido\n",
    "    df_geral = spark.createDataFrame(geral, schema=schema_geral)\n",
    "    df_financeira = spark.createDataFrame(financeira, schema=schema_financeira)\n",
    "    df_mercado = spark.createDataFrame(mercado, schema=schema_mercado)\n",
    "    df_dividendos = spark.createDataFrame(dividendos, schema=schema_dividendos)\n",
    "    df_valuation = spark.createDataFrame(valuation, schema=schema_valuation)\n",
    "\n",
    "    # Converter Unix timestamp para data legível\n",
    "    df_dividendos = df_dividendos.withColumn('data_exdividendo', from_unixtime(col('data_exdividendo').cast('bigint')))\n",
    "    \n",
    "    # Adicionar colunas de timestamp\n",
    "    df_geral = df_geral.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df_geral = df_geral.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "    df_financeira = df_financeira.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df_financeira = df_financeira.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "    df_mercado = df_mercado.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df_mercado = df_mercado.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "    df_dividendos = df_dividendos.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df_dividendos = df_dividendos.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "    df_valuation = df_valuation.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "    df_valuation = df_valuation.withColumn('dthr_igtao', current_timestamp())\n",
    "    \n",
    "    return df_geral, df_financeira, df_mercado, df_dividendos, df_valuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral, df_financeira, df_mercado, df_dividendos, df_valuation = criar_tabelas_spark(df_tickers)\n",
    "# Exibir os DataFrames\n",
    "df_geral.show()\n",
    "df_financeira.show()\n",
    "df_mercado.show()\n",
    "df_dividendos.show()\n",
    "df_valuation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retorno_mensal.printSchema()\n",
    "df_ativas.printSchema()\n",
    "df_geral.printSchema()\n",
    "df_financeira.printSchema()\n",
    "df_mercado.printSchema()\n",
    "df_dividendos.printSchema()\n",
    "df_valuation.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 3 - Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {\n",
    "    '^BVSP': 'Ibovespa',\n",
    "    'BOVA11.SA': 'BOVA11',\n",
    "    '^GSPC': 'S&P 500',\n",
    "    '^DJI': 'Dow Jones',\n",
    "    '^IXIC': 'NASDAQ',\n",
    "    '^FTSE': 'FTSE 100',\n",
    "    '^GDAXI': 'DAX 30',\n",
    "    '^N225': 'Nikkei 225'\n",
    "}\n",
    "\n",
    "# Definindo o esquema do DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"indice\", StringType(), True),\n",
    "    StructField(\"abertura\", DoubleType(), True),\n",
    "    StructField(\"alta\", DoubleType(), True),\n",
    "    StructField(\"baixa\", DoubleType(), True),\n",
    "    StructField(\"fechamento\", DoubleType(), True),\n",
    "    StructField(\"fechamento_ajustado\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True),  # Alterado para LongType\n",
    "    StructField(\"nome_indice\", StringType(), True),\n",
    "    StructField(\"data\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Criar uma lista para armazenar os dados\n",
    "data_list = []\n",
    "\n",
    "# Adicionando dados históricos (exemplo)\n",
    "for ticker, name in indices.items():\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date_str, end=end_date_str)\n",
    "        if not data.empty:\n",
    "            for index, row in data.iterrows():\n",
    "                # Converter os valores para float ou inteiro antes de adicionar à lista\n",
    "                data_list.append((ticker, float(row['Open']), float(row['High']), float(row['Low']), \n",
    "                                  float(row['Close']), float(row['Adj Close']), int(row['Volume']),  # Converte para inteiro\n",
    "                                  name, index.date()))  # Usa index.date() para pegar apenas a parte da data\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao baixar dados para {ticker}: {e}\")\n",
    "\n",
    "# Criar um DataFrame Spark\n",
    "df_indices = spark.createDataFrame(data_list, schema)\n",
    "\n",
    "janela = Window.partitionBy('indice').orderBy('data')\n",
    "\n",
    "# Calcular preço de fechamento do mês anterior (deslocar uma linha para cima)\n",
    "df_indices = df_indices.withColumn('fechamento_mes_anterior', lag('fechamento').over(janela))\n",
    "\n",
    "# Calcular Retorno em valor (diferença absoluta)\n",
    "df_indices = df_indices.withColumn('valor_retorno', col('fechamento') - col('fechamento_mes_anterior'))\n",
    "\n",
    "# Calcular Retorno em porcentagem\n",
    "df_indices = df_indices.withColumn('porcentagem_retorno', (col('valor_retorno') / col('fechamento_mes_anterior')) * 100)\n",
    "\n",
    "# Adicionar colunas dt_ptcao e dthr_igtao\n",
    "df_indices = df_indices.withColumn('dt_ptcao', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "df_indices = df_indices.withColumn('dthr_igtao', current_timestamp())\n",
    "\n",
    "# Exibir o DataFrame final\n",
    "df_indices = df_indices.orderBy('indice','data')\n",
    "df_indices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRVNAME = \"postgres\"\n",
    "USER = \"airflow\"\n",
    "PASSWORD = \"airflow\"\n",
    "HOST = \"postgres\"\n",
    "PORT = \"5432\"\n",
    "DBNAME = \"paxpedb\"\n",
    "\n",
    "# Parâmetros de conexão usando as variáveis\n",
    "conn_params = {\n",
    "    'dbname': DBNAME,\n",
    "    'user': USER,\n",
    "    'password': PASSWORD,\n",
    "    'host': HOST,\n",
    "    'port': PORT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection():\n",
    "    try:\n",
    "        # Connect to the PostgreSQL server using variables\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=SRVNAME,\n",
    "            user=USER,\n",
    "            password=PASSWORD,\n",
    "            host=HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        print(\"Conexão com postgres sucedida\")\n",
    "        return True\n",
    "    except OperationalError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "if not test_connection():\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### para inserir no postgres é necessário\n",
    "> 1. alimentar as tabelas em stage --- temp_nometabela\n",
    "> 2. realizar o upsert comparando as tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(df, table_name, schema_name=\"paxpestg\"):\n",
    "  \n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{HOST}:{PORT}/{DBNAME}\") \\\n",
    "        .option(\"dbtable\", f\"{schema_name}.{table_name}\") \\\n",
    "        .option(\"user\", USER) \\\n",
    "        .option(\"password\", PASSWORD) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escrita das tabelas em stage\n",
    "write_to_postgres(df_retorno_mensal, \"temp_retorno_mensal\")\n",
    "write_to_postgres(df_ativas, \"temp_captacao_mercado\")\n",
    "write_to_postgres(df_geral, \"temp_cadastro\")\n",
    "write_to_postgres(df_financeira, \"temp_financas\")\n",
    "write_to_postgres(df_mercado, \"temp_mercado\")\n",
    "write_to_postgres(df_dividendos, \"temp_dividendos\")\n",
    "write_to_postgres(df_valuation, \"temp_valuation\")\n",
    "write_to_postgres(df_indices, \"temp_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_data(table_name, temp_table_name, key_columns):\n",
    "    try:\n",
    "        # Conectar ao PostgreSQL\n",
    "        connection = psycopg2.connect(**conn_params)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Definir o fuso horário da sessão para UTC-3\n",
    "        cursor.execute(\"SET TIME ZONE 'America/Sao_Paulo';\")\n",
    "\n",
    "        # Definir esquemas\n",
    "        schema_fact = \"paxpe\"\n",
    "        schema_stg = \"paxpestg\"\n",
    "        \n",
    "        # Obter todas as colunas da tabela\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{schema_fact}' AND table_name = '{table_name}'\n",
    "        \"\"\")\n",
    "        all_columns = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        # Identificar colunas não chave e colunas de referência\n",
    "        non_key_columns = [col for col in all_columns if col not in key_columns and col not in ['dt_ptcao', 'dthr_igtao']]\n",
    "        reference_columns = ['dthr_igtao']\n",
    "\n",
    "        # Construir a declaração SQL de atualização col1, col2\n",
    "        key_columns_str = ', '.join(key_columns)\n",
    "        update_set_non_key = ', '.join([\n",
    "            f\"{col} = EXCLUDED.{col}\" \n",
    "            for col in non_key_columns\n",
    "        ])\n",
    "        \n",
    "        # Atualizar apenas as colunas não-chave se houver uma mudança\n",
    "        where_condition = ' OR '.join([\n",
    "            f\"target.{col} IS DISTINCT FROM EXCLUDED.{col}\" \n",
    "            for col in non_key_columns\n",
    "        ])\n",
    "        \n",
    "        # Para atualizar as colunas de referência se houver uma atualização nas colunas não-chave\n",
    "        update_set_reference = ', '.join([\n",
    "            f\"{col} = EXCLUDED.{col}\" \n",
    "            for col in reference_columns\n",
    "        ])\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO {schema_fact}.{table_name} \n",
    "        (SELECT * FROM {schema_stg}.{temp_table_name})\n",
    "        ON CONFLICT ({key_columns_str}) \n",
    "        DO UPDATE SET\n",
    "        {update_set_non_key},\n",
    "        {update_set_reference}\n",
    "        WHERE EXISTS (\n",
    "            SELECT 1\n",
    "            FROM {schema_fact}.{table_name} AS target\n",
    "            WHERE { ' AND '.join([f\"target.{key} = EXCLUDED.{key}\" for key in key_columns]) }\n",
    "            AND ({where_condition})\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Executar a declaração SQL de upsert\n",
    "        cursor.execute(sql)\n",
    "        connection.commit()\n",
    "        print(f\"Upsert de {schema_stg}.{temp_table_name} realizado para {schema_fact}.{table_name}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colunas chave para cada tabela\n",
    "key_columns_retorno_mensal = [\"ticker\", \"data\"]\n",
    "key_columns_cadastro = [\"ticker\"]\n",
    "key_columns_cap_mercado = [\"ticker\"]\n",
    "key_columns_financas = [\"ticker\"]\n",
    "key_columns_mercado = [\"ticker\"]\n",
    "key_columns_dividendos = [\"ticker\",\"data_exdividendo\"]\n",
    "key_columns_valuation = [\"ticker\"]\n",
    "key_columns_indices = [\"indice\",\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realiza o upsert\n",
    "upsert_data(\"retorno_mensal\", \"temp_retorno_mensal\", key_columns_retorno_mensal)\n",
    "upsert_data(\"cadastro\", \"temp_cadastro\", key_columns_cadastro)\n",
    "upsert_data(\"captacao_mercado\", \"temp_captacao_mercado\", key_columns_cap_mercado)\n",
    "upsert_data(\"financas\", \"temp_financas\", key_columns_financas)\n",
    "upsert_data(\"mercado\", \"temp_mercado\", key_columns_mercado)\n",
    "upsert_data(\"dividendos\", \"temp_dividendos\", key_columns_dividendos)\n",
    "upsert_data(\"valuation\", \"temp_valuation\", key_columns_valuation)\n",
    "upsert_data(\"indices\", \"temp_indices\", key_columns_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
